{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Data Loading"
      ],
      "metadata": {
        "id": "OU-_BaNRrJBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "data = pd.read_excel(\"/content/drive/MyDrive/data sets/SPAM EMAIL DETECTION WITH MACHINE LEARNING/Spam Email Detection.xlsx\")\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAauS87-rHKJ",
        "outputId": "68493b37-f1f0-4718-a50a-24953d4fb15f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     v1                                                 v2 Unnamed: 2  \\\n",
            "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
            "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
            "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
            "\n",
            "  Unnamed: 3 Unnamed: 4  \n",
            "0        NaN        NaN  \n",
            "1        NaN        NaN  \n",
            "2        NaN        NaN  \n",
            "3        NaN        NaN  \n",
            "4        NaN        NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that only columns \"v1\" and \"v2\" contain meaningful data, while the other columns (\"Unnamed: 2\", \"Unnamed: 3\", and \"Unnamed: 4\") appear to be empty.\n",
        "\n"
      ],
      "metadata": {
        "id": "DvJrKLFmrnDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Data Preprocessing"
      ],
      "metadata": {
        "id": "9ls0J-CtyYbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2.1: Data Cleaning\n",
        "\n",
        "We'll start by dropping the unnecessary columns (\"Unnamed: 2\", \"Unnamed: 3\", and \"Unnamed: 4\") and renaming the remaining columns to something more descriptive.\n",
        "\n"
      ],
      "metadata": {
        "id": "_ePLXM5gyju3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns\n",
        "data.drop(columns=[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], inplace=True)\n",
        "\n",
        "# Rename columns\n",
        "data.columns = [\"label\", \"email\"]\n",
        "\n",
        "# Display the first few rows of the DataFrame after preprocessing\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ygrg4uz4rosi",
        "outputId": "462f820a-8f8c-4bc0-ae9a-57675a2aed4a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  label                                              email\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DataFrame now contains two columns: \"label\" and \"email\", with meaningful data.\n",
        "\n",
        "Next, we'll proceed with text preprocessing to clean and prepare the text data for further analysis and modeling."
      ],
      "metadata": {
        "id": "7BUfjbcsyatL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Text Preprocessing"
      ],
      "metadata": {
        "id": "MLvFrUq7yzSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3.1: Text Cleaning\n",
        "\n",
        "We'll clean the text data by removing any punctuation, converting text to lowercase, and tokenizing the text into individual words."
      ],
      "metadata": {
        "id": "mtvV2Wr9y32s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define function for text cleaning\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Tokenize the text\n",
        "        tokens = word_tokenize(text)\n",
        "        # Convert to lowercase\n",
        "        tokens = [word.lower() for word in tokens]\n",
        "        # Remove punctuation\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        stripped = [word.translate(table) for word in tokens]\n",
        "        # Remove non-alphabetic tokens\n",
        "        words = [word for word in stripped if word.isalpha()]\n",
        "        # Remove stop words\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [word for word in words if not word in stop_words]\n",
        "        # Join the words back into a string\n",
        "        cleaned_text = ' '.join(words)\n",
        "        return cleaned_text\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "\n",
        "# Apply text cleaning to the 'email' column\n",
        "data['cleaned_email'] = data['email'].apply(clean_text)\n",
        "\n",
        "# Display the first few rows of the DataFrame after text preprocessing\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lab62XQVyyVN",
        "outputId": "8a7deb29-3b1f-4c19-f289-d2c7455b6059"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  label                                              email  \\\n",
            "0   ham  Go until jurong point, crazy.. Available only ...   \n",
            "1   ham                      Ok lar... Joking wif u oni...   \n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
            "3   ham  U dun say so early hor... U c already then say...   \n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
            "\n",
            "                                       cleaned_email  \n",
            "0  go jurong point crazy available bugis n great ...  \n",
            "1                            ok lar joking wif u oni  \n",
            "2  free entry wkly comp win fa cup final tkts may...  \n",
            "3                u dun say early hor u c already say  \n",
            "4          nah nt think goes usf lives around though  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text cleaning process seems to have worked fine, and now the DataFrame contains a new column named \"cleaned_email\" with the preprocessed text.\n",
        "\n",
        "Next, we'll proceed with feature engineering by converting the cleaned text into numerical features using the TF-IDF vectorizer."
      ],
      "metadata": {
        "id": "miFTkp2-zeCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Feature Engineering\n"
      ],
      "metadata": {
        "id": "r56UyMDrziYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer to convert the cleaned text into numerical features."
      ],
      "metadata": {
        "id": "_aGg9xUsVgZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=3000)  # You can adjust max_features as needed\n",
        "\n",
        "# Fit and transform the cleaned email text\n",
        "X = vectorizer.fit_transform(data['cleaned_email'])\n",
        "\n",
        "# Convert to DataFrame for better inspection\n",
        "X_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the feature matrix\n",
        "print(X_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxLbmNhpzh2v",
        "outputId": "f9c9c67a-7c6a-4475-86ba-f4bdd91ac8e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   aah  aathi  abi  ability  abiola  abj  able  absolutly  abt  abta  ...  \\\n",
            "0  0.0    0.0  0.0      0.0     0.0  0.0   0.0        0.0  0.0   0.0  ...   \n",
            "1  0.0    0.0  0.0      0.0     0.0  0.0   0.0        0.0  0.0   0.0  ...   \n",
            "2  0.0    0.0  0.0      0.0     0.0  0.0   0.0        0.0  0.0   0.0  ...   \n",
            "3  0.0    0.0  0.0      0.0     0.0  0.0   0.0        0.0  0.0   0.0  ...   \n",
            "4  0.0    0.0  0.0      0.0     0.0  0.0   0.0        0.0  0.0   0.0  ...   \n",
            "\n",
            "    yr  yrs  yummy  yun  yunny  yuo  yup  zed  zindgi  zoe  \n",
            "0  0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0     0.0  0.0  \n",
            "1  0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0     0.0  0.0  \n",
            "2  0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0     0.0  0.0  \n",
            "3  0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0     0.0  0.0  \n",
            "4  0.0  0.0    0.0  0.0    0.0  0.0  0.0  0.0     0.0  0.0  \n",
            "\n",
            "[5 rows x 3000 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Splitting the Dataset"
      ],
      "metadata": {
        "id": "gJyboF7zV-ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "split the dataset into training and testing sets. We'll use the cleaned email text as our features X and the labels as our target y."
      ],
      "metadata": {
        "id": "41_FdABcWOeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the target variable\n",
        "y = data['label']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the shapes of the training and testing sets\n",
        "print(f'Shape of X_train: {X_train.shape}')\n",
        "print(f'Shape of X_test: {X_test.shape}')\n",
        "print(f'Shape of y_train: {y_train.shape}')\n",
        "print(f'Shape of y_test: {y_test.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpI8XU7gzTFZ",
        "outputId": "9e9e5517-432e-4203-f8fa-fe8bc349a04c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (4457, 3000)\n",
            "Shape of X_test: (1115, 3000)\n",
            "Shape of y_train: (4457,)\n",
            "Shape of y_test: (1115,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has been successfully split into training and testing sets."
      ],
      "metadata": {
        "id": "4K37pgfKWhCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Model Training"
      ],
      "metadata": {
        "id": "MDMWxObUWZbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train a Logistic Regression model on the training data. Logistic Regression is a commonly used algorithm for binary classification problems like spam detection."
      ],
      "metadata": {
        "id": "O6ENzzXBWbxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Display the first few predictions\n",
        "print(y_pred[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0tikPjmWm_L",
        "outputId": "1cfbe631-31c7-4f06-c7dc-2924b48c73a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ham' 'ham' 'ham' 'ham' 'spam' 'ham' 'ham' 'ham' 'ham' 'ham']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has been trained and has made some predictions. Now, let's evaluate the model's performance on the test set."
      ],
      "metadata": {
        "id": "heRgmJ8IWynk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 7: Model Evaluation"
      ],
      "metadata": {
        "id": "1iBn14gUW3mp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "use common evaluation metrics such as accuracy, precision, recall, and F1-score to assess the performance of our spam detection model."
      ],
      "metadata": {
        "id": "e73qekj_W52P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, y_pred, pos_label='spam')\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test, y_pred, pos_label='spam')\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred, target_names=['ham', 'spam'])\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-score: {f1}')\n",
        "print('\\nClassification Report:\\n', report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEYe9YrsW0yx",
        "outputId": "9ffd45e4-d085-485c-8347-e6656eee3ad6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9488789237668162\n",
            "Precision: 0.9514563106796117\n",
            "Recall: 0.6533333333333333\n",
            "F1-score: 0.774703557312253\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.95      0.99      0.97       965\n",
            "        spam       0.95      0.65      0.77       150\n",
            "\n",
            "    accuracy                           0.95      1115\n",
            "   macro avg       0.95      0.82      0.87      1115\n",
            "weighted avg       0.95      0.95      0.94      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation Summary\n",
        "\n",
        "### Performance Metrics\n",
        "- **Accuracy**: 0.95\n",
        "- **Precision** (for spam): 0.95\n",
        "- **Recall** (for spam): 0.65\n",
        "- **F1-score** (for spam): 0.77\n",
        "\n",
        "### Summary\n",
        "The model shows high accuracy and precision but a relatively lower recall for the spam class. This means that while the model is good at correctly identifying emails that are spam (precision), it misses a significant number of actual spam emails (recall).\n",
        "\n",
        "### Next Steps: Improvements\n",
        "\n",
        "1. **Balancing the Dataset**:\n",
        "   - The dataset might be imbalanced (more 'ham' than 'spam'). Techniques such as oversampling the minority class (spam) or undersampling the majority class (ham) can help balance the dataset.\n",
        "\n",
        "2. **Try Different Models**:\n",
        "   - Some models might perform better for this specific task. Consider using models like Random Forest, Gradient Boosting, or even deep learning models.\n",
        "\n",
        "3. **Hyperparameter Tuning**:\n",
        "   - Fine-tuning the hyperparameters of the Logistic Regression model or other models can lead to better performance.\n",
        "\n",
        "4. **Feature Engineering**:\n",
        "   - Adding more features or using different vectorization techniques like Word2Vec or BERT embeddings might improve performance.\n",
        "\n",
        "### Next Step: Balancing the Dataset\n",
        "Let's start with balancing the dataset using oversampling and see if it improves the recall for the spam class.\n"
      ],
      "metadata": {
        "id": "J9OweQNiXOYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Balancing the Dataset\n",
        "We will use the SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "fbY2Q4bvXlOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Apply SMOTE to the training data\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check the new class distribution\n",
        "print(y_train_balanced.value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU1Cm50GXsnH",
        "outputId": "c7402c75-e4dc-4a13-d448-ab140b10e544"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "ham     3860\n",
            "spam    3860\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is now balanced with an equal number of 'ham' and 'spam' emails. Let's retrain the Logistic Regression model on this balanced dataset and evaluate its performance again."
      ],
      "metadata": {
        "id": "9lNY_Ik8X3B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Retraining the Model"
      ],
      "metadata": {
        "id": "wF7RD-MVX4Jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll retrain the Logistic Regression model on the balanced dataset and make predictions on the test set."
      ],
      "metadata": {
        "id": "PRmI-CnCYAGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Logistic Regression model\n",
        "model_balanced = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the balanced dataset\n",
        "model_balanced.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_balanced = model_balanced.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
        "precision_balanced = precision_score(y_test, y_pred_balanced, pos_label='spam')\n",
        "recall_balanced = recall_score(y_test, y_pred_balanced, pos_label='spam')\n",
        "f1_balanced = f1_score(y_test, y_pred_balanced, pos_label='spam')\n",
        "report_balanced = classification_report(y_test, y_pred_balanced, target_names=['ham', 'spam'])\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Accuracy: {accuracy_balanced}')\n",
        "print(f'Precision: {precision_balanced}')\n",
        "print(f'Recall: {recall_balanced}')\n",
        "print(f'F1-score: {f1_balanced}')\n",
        "print('\\nClassification Report:\\n', report_balanced)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHBOVL6XYA8c",
        "outputId": "60f4b559-b9a5-4b5b-e135-6c0f89e8b01c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9757847533632287\n",
            "Precision: 0.9072847682119205\n",
            "Recall: 0.9133333333333333\n",
            "F1-score: 0.9102990033222591\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.99      0.99       965\n",
            "        spam       0.91      0.91      0.91       150\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.95      0.95      0.95      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation Summary\n",
        "\n",
        "### Performance Metrics\n",
        "- **Accuracy**: 0.98\n",
        "- **Precision** (for spam): 0.91\n",
        "- **Recall** (for spam): 0.91\n",
        "- **F1-score** (for spam): 0.91\n",
        "\n",
        "### Summary\n",
        "- **Accuracy** improved to 0.98, indicating the model's overall correctness.\n",
        "- **Precision** for spam indicates that 91% of the emails predicted as spam were actually spam.\n",
        "- **Recall** for spam is now 91%, meaning the model correctly identified 91% of the actual spam emails.\n",
        "- **F1-score** for spam, a balance between precision and recall, is now 0.91.\n",
        "\n",
        "### Next Steps\n",
        "1. **Further Hyperparameter Tuning**:\n",
        "   - Fine-tuning hyperparameters of the Logistic Regression model or trying more sophisticated models can help in pushing the model performance even further.\n",
        "\n",
        "2. **Feature Engineering**:\n",
        "   - Adding or refining features could further improve the model's effectiveness.\n",
        "   - For example, leveraging advanced text representations like TF-IDF, Word2Vec, or BERT embeddings might help.\n",
        "\n",
        "3. **Model Deployment**:\n",
        "   - Once satisfied with the performance, the model can be deployed into a production environment for real-time spam detection.\n"
      ],
      "metadata": {
        "id": "bDsZXqT4Yxy_"
      }
    }
  ]
}